# Аналіз даних за допомогою Python

## 1. Імпорт наборів даних
Імпорт даних — це перший крок, який дозволяє завантажити дані у Python для аналізу. Дані можуть бути в різних форматах: CSV, Excel, JSON, SQL або API.
Це початковий етап, необхідний для всіх типів аналізу. Формат даних залежить від джерела.

### Методи імпорту

- CSV: Використовується для роботи з табличними даними. Найпоширеніший формат для зберігання даних.
  ```python
  df = pd.read_csv("data.csv")
  ```

- Excel: Якщо ваші дані зберігаються у вигляді таблиць Excel, цей метод підходить.
  ```python
  df = pd.read_excel("data.xlsx", sheet_name="Sheet1")
  ```

- SQL: Зручно, якщо ваші дані зберігаються в базах даних.
  ```python
  df = pd.read_sql_query("SELECT * FROM table_name", conn)
  ```

- JSON: Використовується для імпорту даних зі складною структурою (наприклад, із веб-API).
  ```python
  df = pd.read_json("data.json")
  ```

## 2. Обробка даних
Обробка даних включає очищення, зміну формату, нормалізацію та бінінг. Це важливий етап, оскільки "сирі" дані зазвичай мають пропуски, шуми або неправильні формати.

Коли використовувати? Коли дані потребують очищення або перетворення для побудови моделей або візуалізації.

### a) Обробка відсутніх даних
Пропущені дані можуть спотворити результати аналізу.
```python
df['column_name'].fillna(df['column_name'].mean(), inplace=True)  # Заповнення пропусків середнім
```

### b) Форматування даних
Зміна типу даних для відповідності формату аналізу. Наприклад, перетворення тексту на числа.
```python
df['column_name'] = df['column_name'].astype('float')
```

### c) Нормалізація даних
Змінні різного масштабу (наприклад, ціна в доларах і вага в кілограмах) можуть впливати на модель. Нормалізація вирівнює їх:
```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
df['normalized_column'] = scaler.fit_transform(df[['original_column']])
```

### d) Бінінг (групування)
Розбиття числових значень на категорії (наприклад, "низький", "середній", "високий").
```python
bins = [0, 10, 20, 30]
labels = ['low', 'medium', 'high']
df['binned_column'] = pd.cut(df['column_name'], bins=bins, labels=labels)
```

## 3. Розвідувальний аналіз даних (EDA)
EDA — це процес вивчення даних для пошуку закономірностей, виявлення аномалій та оцінки статистичних зв’язків.

Коли використовувати? Для аналізу взаємозв’язків між змінними до побудови моделі.

### a) Візуалізація
Графіки допомагають швидко зрозуміти структуру даних.

- Використання matplotlib:
Побудова гістограм для розподілу даних:
  ```python
  df['column_name'].hist()
  ```

- Використання seaborn:
Теплова карта для візуалізації кореляції:
  ```python
  sns.heatmap(df.corr(), annot=True)
  ```

### b) Кореляція
Визначає силу залежності між змінними. Значення варіюється від -1 (сильна негативна) до 1 (сильна позитивна).
```python
df.corr()
```

### c) Тест хі-квадрат
Використовується для оцінки статистичної залежності між категоріальними змінними.
```python
chi2, p, dof, expected = chi2_contingency(pd.crosstab(df['col1'], df['col2']))
```


## 4. Розробка моделі
Створення моделі, яка передбачає залежність між змінними на основі тренувальних даних.

Коли використовувати? Коли є залежність між змінними, і потрібно створити передбачувальну модель.

### a) Лінійна регресія
Підходить для аналізу зв’язку між однією незалежною та залежною змінною.
```python
lm = LinearRegression()
lm.fit(X, Y)
````

### b) Множинна лінійна регресія
Використовується, якщо є декілька незалежних змінних.
```python
X = df[['var1', 'var2', 'var3']]
lm.fit(X, Y)
```

### c) Поліноміальна регресія
Моделює нелінійні залежності між змінними.
```python
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
lm.fit(X_poly, Y)
```

### d) Трубопроводи (pipelines)
Спрощує обробку даних та побудову моделі.
```python
pipeline = Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())])
pipeline.fit(X, Y)
```


## 5. Оцінка та вдосконалення моделі
Цей етап перевіряє, наскільки добре модель відповідає даним.

Коли використовувати? Для вибору оптимальних параметрів моделі або вдосконалення точності.

### a) Метрики оцінки
- R² (коефіцієнт детермінації): Відображає, яку частину варіації залежної змінної пояснює модель.
  ```python
  r2 = model.score(X, Y)
  ```

- MSE (середньоквадратична помилка): Вимірює середнє відхилення передбачень від реальних значень.
  ```python
  from sklearn.metrics import mean_squared_error
  mse = mean_squared_error(Y, model.predict(X))
  ```

### b) Хребтова регресія
Регуляризація для уникнення перенавчання.
```python
ridge = Ridge(alpha=1.0)
ridge.fit(X, Y)
```

### c) Сітковий пошук (Grid Search)
Автоматично знаходить найкращі параметри для моделі.
```python
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(ridge, {'alpha': [0.1, 1.0, 10.0]})
grid.fit(X, Y)
```



